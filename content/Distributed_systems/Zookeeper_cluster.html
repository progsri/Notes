<h2> <b> ZooKeeper cluster (ensemble) </b> </h2>
<hr>

<p> Zookeeper cluster is also called zookeeper ensemble. </p>

<h4> <b> Basic setup  </b> </h4>
<p> Create 3 virtual machines and download the ZooKeeper binaries into each VM.  <p>
<hr>

<h4> <b> Configuration </b> </h4>
<p> in <code> conf </code> directory of the zookeeper create zoo.cfg as below for each of the 3 VMs. 192.168.122.211 is the ip of VM 1, 192.168.122.109 is the 
  ip of vm 2 and 192.168.122.157 is the ip of vm 3.
</p>

<pre class="line-numbers language-shell"><code>
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=./data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
#metricsProvider.httpPort=7000
#metricsProvider.exportJvmInfo=true
server.1=192.168.122.211:2888:3888
server.2=192.168.122.109:2888:3888
server.3=192.168.122.157:2888:3888

</code></pre>

<p> 
  under the dataDir given in the above conf create a file <code> myid </code> and write 1 in vm 1, 2 in vm 2 and 3 in vm 3.
</p>
<hr>

<h4> <b> Start the cluster </b> </h4>
<p> For each of the VM, <code> ./bin/zkServer.sh start  </code> would start the zookeeper in that vm and will connect with  VM cluster.</p>
<hr>

<h4> <b> Test if the cluster formed </b> </h4>
<p> connect to zookeeper in vm 1 <code> /zkCli.sh -server 192.168.122.211 </code> or connect to all 3 zookeepers using 
  <code> ./zkCli.sh -server 192.168.122.211,192.168.122.109,192.168.122.157 </code>.</p>
<p> Port number used by client to connect to individual zookeeper is mentioned in <code> zoo.cfg </code> as <code> clientPort </code> </p>
<p> create a znode </p>
<pre class="line-numbers language-shell"><code>
[zk: 192.168.122.211(CONNECTED) 1] ls /
[zookeeper]
[zk: 192.168.122.211(CONNECTED) 2] create /test
Created /test
[zk: 192.168.122.211(CONNECTED) 3] ls /
[test, zookeeper]
</code></pre>

<p> Kill the zookeeper in vm 1 using <code> kill -9 pid_of_zookeeper </code> or <code> /bin/zkServer.sh stop </code> </p>
<p> connect to zookeeper in vm 2 <code> /zkCli.sh -server 192.168.122.109 </code> </p>
<p> verify if the znode created from vm1 still exists </p>
<pre class="line-numbers language-shell"><code>
[zk: 192.168.122.109(CONNECTED) 0] ls /
[test, zookeeper]
</code></pre>

<p> This proves that the zookeeper cluster works and the data ( in this case znode ) is being replicated </p>

<hr>

<h4> <b> Find a instance is leader or follower </b> </h4>
<p> The below command provides the <b> mode </b> of the instance as either <code> leader </code> or <code> follower </code> </p>
<pre class="line-numbers language-shell"><code>
node@dns:~/ENGINE/Zookeeper/apache-zookeeper-3.6.2-bin$ ./bin/zkServer.sh status
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /home/node/ENGINE/Zookeeper/apache-zookeeper-3.6.2-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
</code></pre>
<hr>
<!---------------------------------------------->

<hr>
