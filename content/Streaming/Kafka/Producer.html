<h2><strong> Producer </strong></h2>
<hr>


<h4><strong> Connection Pooling </strong></h4>
<p> The library provided by kafka itself takes care of connection pool just like <b> jedis library  </b> takes care for redis.
</p>
<p class="text-danger">
  The producer instance is thread safe, which means the same instance can be used across multiple sends.
</p>

<hr>
<h4><strong> Simple </strong></h4>
<p class="text-danger"> What is the need and usage of giving a ProducerConfig.TRANSACTIONAL_ID_CONFIG </p>

<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class Producer {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            producer.send((record)); // This is async

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}
</code></pre>

<hr>
<h4><strong> With Callbacks  </strong></h4>

<p> Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
 also we would know that and accordingly we can take steps..may be logged the error and retry later.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<pre class="language-shell line-numbers"><code>
offset 8
partition 0
</code></pre>


<hr>
<h4><strong> Using key </strong></h4>
<p>
  First time when a message is written to kafka with a key, kafka remembers the random partion it has the written that msg to.
  So, for next message with the same key, kafka writes to the same partition, as the order of message is maintained in a
  partition, this can be made use of.
</p>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        // key ensure the message is always written to the same partition
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-6","key-2","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<hr>
<h4><strong> Retries </strong></h4>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>


<hr>
<h4><strong> Idempotent </strong></h4>
<p>
    Let say the producer sends the message to kafka, kafka when trying to send ack bakc gets network failure. Producer 
    thinks the message failed and if retires is enabled tries to send again, so we get duplicate messages. In order to 
    avoid this we need to set <b>enable.idempotence </b> property
</p>.


<hr>
<h4><strong> Compression </strong></h4>
<p>
 When we compress using Gzip, snappy or lz4 we reduce the size of the data being transmitted from producer to kafka and 
 between replicas as well. Also use less space to store the message. But it might slightly increase the time to send or 
 receive msg as at the client it has to compress or decompress. Can be set as a property. 
</p>

<hr>
<h4><strong> Batching </strong></h4>
<p>
  By default producer does <b> smart batching </b>. As send is assync in nature it always be default tries to batch the 
  messages, it waits for the before batch ack and once it receives them sends the next batch. We can control this behavour
  with <b> linger.ms </b> nd <b> batch.size </b>
</p>
<p>
  linger.ms = 0 by default which means smart batching happens it only waits for the ack., but if you want to wait more time
  before sending the batch out we can control with this property.Having a little linger is good might be better sometimes,
  it can to be cacluated based on the application. It would not have an effected if the batch reaches batch.size.
</p>
<p>
  batch.size is default to 16 KB, this is the size of the batch we can wait before sending the batch out. if batch.size
  does not reach and linger.ms has reached then the batch will sent out. <mark> batch.size is per partition </mark>
</p>




