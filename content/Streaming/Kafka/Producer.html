<h2><strong> Producer </strong></h2>
<hr>


<h4><strong> Connection Pooling </strong></h4>
<p> The library provided by kafka itself takes care of connection pool just like <b> jedis library  </b> takes care for redis.
</p>
<p class="text-danger">
  The producer instance is thread safe, which means the same instance can be used across multiple sends.
</p>

<hr>
<h4><strong> Simple </strong></h4>
<p class="text-danger"> What is the need and usage of giving a ProducerConfig.TRANSACTIONAL_ID_CONFIG </p>

<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class Producer {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            producer.send((record)); // This is async

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}
</code></pre>

<hr>
<h4><strong> With Callbacks  </strong></h4>

<p> Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
 also we would know that and accordingly we can take steps..may be logged the error and retry later.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<pre class="language-shell line-numbers"><code>
offset 8
partition 0
</code></pre>


<hr>
<h4><strong> Using key </strong></h4>
<p>
  First time when a message is written to kafka with a key, kafka remembers the random partion it has the written that msg to.
  So, for next message with the same key, kafka writes to the same partition, as the order of message is maintained in a
  partition, this can be made use of.
</p>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        // key ensure the message is always written to the same partition
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-6","key-2","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<hr>
<h4><strong> Retries </strong></h4>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>


<hr>
<h4><strong> Idempotent </strong></h4>
<p>
    Let say the producer sends the message to kafka, kafka when trying to send ack bakc gets network failure. Producer 
    thinks the message failed and if retires is enabled tries to send again, so we get duplicate messages.
</p>.
