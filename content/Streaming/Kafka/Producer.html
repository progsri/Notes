<h2><strong> Producer </strong></h2>
<hr>

<h4><strong> Observations </strong> </h4>
<p>
   Producer sends messages to a topic to any broker.They do not choose a particular broker or partition. <mark> Broker discovery
 </mark> is the way any broker would know any other broker info and the metadata such as topics and which broker they are present,
 topics etc. So, the broker which the producer is connected would now the reqquired information by broker discovery.
</p>
<p>
 When producer writes to a topic, it can set acknowledgement for the write.
 <ul>
  <li>ack = 0 indicates the producer does not wait for ack, this is fastest but does not guarantee if the message is success</li> 
  <li>ack = 1 indicates the producer  will only wait for the ack from leader, does not gaurantee if the message is replicated,
    so, the broker of the leader copy dies before the message is the replicated, then there is a chance of losing that data.
  </li> 
  <li>ack = all or -1 indicates the producer will wait for the ack from leader and replicas, this would of course be slow.
  min.insync.replicas tells the number of ISR that should acknowledge that they did write/receive the msg, which means it is not
   necessary that all the ISR should write/receive the msg.
  </li> 
 </ul>
</p>
<p>
 Producer can write to the same partition by using a key, otherwise writes go to different partitions using round
 robin algorithm. When partions are added or removed the key will no longer guarantee that it would go to the same partitions as the 
 partition to send the msg for a key is determined by <code> Utils.abs(Utils.murmur2(record.Key())) % num of partitions</code>.
 murmur2 is the hashing alogorithm.
</p>
<p>
 using <b> transactions </b> we can either rollback or commit the transaction, this is useful when writing to multiple topics.
</p>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>
<p>
  When done programmatically library sends messages in batch, see Producer page. 
</p>

<h4><strong> Connection Pooling </strong></h4>
<p> The library provided by kafka itself takes care of connection pool just like <b> jedis library  </b> takes care for redis.
</p>
<p class="text-danger">
  The producer instance is thread safe, which means the same instance can be used across multiple sends.
</p>

<hr>
<h4><strong> Simple </strong></h4>
<p class="text-danger"> What is the need and usage of giving a ProducerConfig.TRANSACTIONAL_ID_CONFIG </p>

<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class Producer {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            producer.send((record)); // This is async

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}
</code></pre>

<hr>
<h4><strong> With Callbacks  </strong></h4>

<p> Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
 also we would know that and accordingly we can take steps..may be logged the error and retry later.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-4","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<pre class="language-shell line-numbers"><code>
offset 8
partition 0
</code></pre>


<hr>
<h4><strong> Using key </strong></h4>
<p>
  First time when a message is written to kafka with a key, kafka remembers the random partion it has the written that msg to.
  So, for next message with the same key, kafka writes to the same partition, as the order of message is maintained in a
  partition, this can be made use of.
</p>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>
<p>
 When partions are added or removed the key will no longer guarantee that it would go to the same partitions as the 
 partition to send the msg for a key is determined by <code> Utils.abs(Utils.murmur2(record.Key())) % num of partitions</code>.
 murmur2 is the hashing alogorithm.
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ProducerWithCallbacks {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"id-1");  //????

        // one String for property key and antoher string for property value
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);

        // record to produce which takes topic and the message
        // key ensure the message is always written to the same partition
        ProducerRecord<String, String> record = new ProducerRecord<>("topic-6","key-2","value1");

        try{
            producer.initTransactions(); /// ???
            producer.beginTransaction();

            // This is async
            // Call back gives you more control , if the msg is produced then we get the other metadata, if it fails
            // also we would know that and accordingly we can take steps..may be logged the error and retry later.
            producer.send((record), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                   if(exception == null){
                       System.out.println("offset " +  metadata.offset());
                       System.out.println("partition " + metadata.partition());
                   }else{
                       System.out.println("Message was not produced");
                   }
                }
            });

            producer.commitTransaction();
        }catch(Exception ex){
            System.out.println("Error " + ex.getMessage());
            producer.abortTransaction();
        }finally{
            System.out.println("Flushing... ");
            producer.flush(); //As send is async ..this would force to flush the data to kafka
            producer.close();
        }
    }
}

</code></pre>

<hr>
<h4><strong> Retries </strong></h4>
<p>
  In case of failures, kafka retries for a very high value which can be configured for every <mark> retry.backoff.ms </mark>
 secs till a timeout of <mark> delivery.timeout.ms </mark>. However the order of message gets effected, if we are based on 
 key based writes then  this woul be a issue.
</p>


<hr>
<h4><strong> Idempotent </strong></h4>
<p>
    Let say the producer sends the message to kafka, kafka when trying to send ack bakc gets network failure. Producer 
    thinks the message failed and if retires is enabled tries to send again, so we get duplicate messages. In order to 
    avoid this we need to set <b>enable.idempotence </b> property
</p>


<hr>
<h4><strong> Compression </strong></h4>
<p>
 When we compress using Gzip, snappy or lz4 we reduce the size of the data being transmitted from producer to kafka and 
 between replicas as well. Also use less space to store the message. But it might slightly increase the time to send or 
 receive msg as at the client it has to compress or decompress. Can be set as a property. 
</p>
<p>
  snappy has high CPU/compression ration. Designed by google.
</p>

<hr>
<h4><strong> Batching </strong></h4>
<p>
  By default producer does <b> smart batching </b>. As send is assync in nature it always be default tries to batch the 
  messages, it waits for the before batch ack and once it receives them sends the next batch. We can control this behavour
  with <b> linger.ms </b> nd <b> batch.size </b>
</p>
<p>
  linger.ms = 0 by default which means smart batching happens it only waits for the ack., but if you want to wait more time
  before sending the batch out we can control with this property.Having a little linger is good might be better sometimes,
  it can to be cacluated based on the application. It would not have an effected if the batch reaches batch.size.
</p>
<p>
  batch.size is default to 16 KB, this is the size of the batch we can wait before sending the batch out. if batch.size
  does not reach and linger.ms has reached then the batch will sent out. <mark> batch.size is per partition </mark>
</p>

<hr>
<h4><strong> Very high sends or not responsive broker </strong></h4>
<p>
  when Producer sends very high messages to kafka and kafka could not keep up with the volume or if kafka is not responding
  and Producer keeps sending messages, all these messages are sent asynchronously which means they end up in a batch at 
  Producer whose size is <b> buffer.memory </b>, so if kafka is not responding or is not able to keep up with the send 
  rate then after <b> max.block.ms </b> time the Producer would throw an exception or if the buffer.memory is reached before 
  max.block.ms is reached then also Producer would throw an exception.
</p>


