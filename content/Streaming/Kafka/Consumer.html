<hr>
<h4><strong> Consumer </strong> </h4>
<p>
   Consumer gets messages from a topic from any broker it is connected to.They do not choose a particular broker or partition. <mark> Broker discovery
 </mark> is the way any broker would know any other broker info and the metadata such as topics and which broker they are present,
 topics etc. So, the broker which the consumer is connected would now the required information by broker discovery.
</p>
<p>
 For a particular partition, order of messages is gauranteed but there can always be producer retries or complete failures to
 write.
</p>
<p>
 Lets say we hava topic that has 3 partitions, how would one consumber read from 3 partitions at the same time or
 <span class="text-danger">Can it ? </span>. For this we use consumer groups which contains of group of consumers, each consumer
 can read from one or mode partitions, but never will a partition be read by 2 or more consumers in a consumber group, this 
 way the partitions can be consumer in parallel ( balancing the load ).
</p>
<p>
   Consumer can process a single message or group of messages(batch) at a time. This <u> <a href="https://kafka.apache.org/23/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html">
 Diferent approaches </a> </u>
</p>
<p> This is a pull based i.e the broker does not give us the message until the consumer asks for one. consumer is supposed to
 keep polling. <span class="text-danger"> Polling is inefficient...then why is kafka entertainging this</span>
</p>
<p>
  When done programmatically library receives messages in batch, see Consumer page. 
</p> 
<p>
  consumers are listeners, in other messaging system a consumer or listenser is subscribed to the broker on a topic, 
    if a msg is written to that topic, the broker would send to the listener if it is avaialable, in case the listener is not
    available then the message is lost unless we have storage mechanism <span class="text-danger"> even in this case how do 
    they get back the lost messages </span>. In case of kafka, the broker doest not send, it is the cuty of the consumer to
    pool the messages.
</p>

<hr>
<h4><strong> Consumer Groups</strong> </h4>
<p>
  Helps to load balance the reads. Consumer can have consumber that are not active( they are not reading )
</p>
<p>
  Consists of consumers that read data of topic from multiple partitions parallely. a consumer can read from multiple 
 partitions but a partition cannot be read by multiple consumers in a consumer group. A partition can  be read by multiple
 consumer if each consumer belongs to different consumber group.
</p>

<hr>
<h4><strong> Consumer offset </strong> </h4>
<p>
 Consumer offset tells the broker or cluster which offset in a partition a consumer has read. These are recorded in a topic
 called <b> _consumer_offset </b>
</p>
<p>
 consumer tells the broker the read offset by 3 modes:
<ul>
 <li> at most once ... when it receives the message but if it dies while processing, this would be not considered for reprocessing</li>
  <li> at least once ... when it received and processed .... this is the safest.</li>
 <li> <span class="text-danger">Exactly once </span> </li>
</ul>
</p>
<p>
 Consumer offset is just a way to telling the broker that a consumer is done reading that message from a particular partion.
 so that when this consumer needs another message or a new consumer is starting to read from the same partion, kafka would
 know where to start from. Whether it is a single consumer in default group assigned by kafka or group of consumer in a 
 consumer group, offsets can be reset for a particular topic in the group or all topics in the group.
</p>
<p>
  To display error message if a sub component fails, use <ErrorBoundary> and ComponentDidCatch lifecycle, this will display
 a default error html page instead of react's error page.
</p>
 <p>
  <b> Reading from a partition </b> assign and seek go in combination. assign is about the partitions it should read from and seek is the offset in the 
  partition. When the consumer polls it would pool from this partition and offset.
</p>


<h4><strong> Connection Pooling </strong></h4>
<p> The library provided by kafka itself takes care of connection pool just like <b> jedis library  </b> takes care for redis.
</p>

<hr>
<h4><strong> Simple </strong></h4>
<p class="text-danger"> Is not polling inefficient ? </p>
<p>
    In the below code we are not subscribing to the broker, in kafka broker does not entertain such things like other
messaging systems, we are subscribing to the consumer library code ( imagine like Rxjava )
</p>
<p>
  we can also subscribe to multiple topics. 
 </p>
<p>
  Observe that the library gives messages in batch ( group of messages )
</p>
<pre class="language-java line-numbers"><code>
package com.ninjashore.kafka.producer;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;

public class Consumer {

    public static void main(String[] args){

        String bootStrapServers = "127.0.0.1:9092";
        Properties properties = new Properties();
        properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,bootStrapServers);
        properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG,"group-1");
        properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(properties);

        consumer.subscribe(Arrays.asList("topic-6"));

        while(true){
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100)); // polling is inefficient ??

            for (ConsumerRecord<String,String> consumerRecord : records){
                System.out.println("key " + consumerRecord.key());
                System.out.println("Value " + consumerRecord.value());
                System.out.println("Partition " + consumerRecord.partition());
                System.out.println("Offset " + consumerRecord.offset());
                System.out.println(" --------------- ");
            }
        }


    }
}

</code></pre>

<hr>
<h4><strong> Rebalancing </strong></h4>
<p>
  When ever a consumer joins or leaves a consumer group, kafka automatically rebalancing the load. 
  For example take a topic with 3 partitons. When the first consumer joins kafka assigns all the 3 partitions to this 
  consumer, when the second consumer joins this group  then kafka rebalances the partitions, it takes away one or more
  partitions from the consumer one and assigns to second consumer. Similarly when a consumer leaves the partitions it was
  assigned would be assigned to other existing consumers in that group.
</p>
<p>
  when rebalancing happens the existing or new consumer would be slow.
</p>

<hr>
<h4><strong> assign and seek  </strong></h4>
<p>
    <b> Reading from a partition </b> assign and seek go in combination. assign is about the partitions it should read from and seek is the offset in the 
  partition. When the consumer polls it would pool from this partition and offset.
</p>


<hr>
<h4><strong> Idempotence  </strong></h4>
<p>
  Sometimes when the consumer processes group of messages and just before acknowleging kafka dies, then the next consumber 
  will reporcess all the messages again. This would cause duplicates on the end system consumer is writing to. So it is always 
  better to use an ID which would help the end systems to know it is a duplicate. One such ID would be combination of topic,
  partition, offset. In case of mongo we can use upsert where if record does not exist it will insert otherwise it would
  update... perfect for idempotence. but <span class="text-danger"> how do we do this for RDBMS ( postgres ) </span>
</p>

<hr>
<h4><strong> Commit strategy  </strong></h4>
<p>
  By default Producer is in auto commit mode, which means after <b> auto.commit.interval.ms </b> it would be commited even if 
  the consumer is till processing. So not a good option.
</p>
<p>
  Manual commit is done by disabling auto commit via property and then we can set the number of records to receive when
  pooling via property  <b> MAX_POOL_RECORD_CONFIG </b>. Only once done we would commit the batch.
</p>


<hr>
<h4><strong> Detecting consumer alive or dead  </strong></h4>
<p>
  all the consumers interact with consumer group coordinator which keeps looking for heartbeats from consumber, a consumer 
  sends heartbead every <b> hearbeat.interval.ms </b>. If the consumber does not send heart beat after <b> session.timeout.ms 
  </b> it is considered as dead and rebalancing of consumers takes place.
</p>
<p>
  if a consumer does not poll every <b> max.poll.interval.ms </b> they it is considered a dead. This tells that a consumer 
  had got choked while processing a batch of messages, for example in big data apps. It is better to process less number of 
  messages for evry poll this can be set using the property <b> MAX_POOL_RECORD_CONFIG </b>.
</p>


